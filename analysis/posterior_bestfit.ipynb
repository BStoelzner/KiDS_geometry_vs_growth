{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from chainconsumer import ChainConsumer\n",
    "import glob\n",
    "import pandas as pd\n",
    "import scipy.linalg\n",
    "from scipy.special import erfcinv\n",
    "import anesthetic\n",
    "import scipy.stats\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from https://github.com/KiDS-WL/kcap/blob/master/utils/stat_tools.py\n",
    "import scipy.optimize\n",
    "\n",
    "def find_CI(method, samples, weights=None, coverage=0.683,\n",
    "            logpost=None, logpost_sort_idx=None,\n",
    "            return_point_estimate=False, return_coverage=False, \n",
    "            return_extras=False, options=None):\n",
    "    \"\"\"Compute credible intervals and point estimates from samples.\n",
    "    Arguments\n",
    "    ---------\n",
    "    method : str\n",
    "        Method to compute CI. Options are \"PJ-HPD\", \"tail CI\", \"std\", and \"HPD\".\n",
    "        PJ-HPD:  Compute the CI from the joint posterior HPD region such that\n",
    "                 the projected range of the HPDR has coverage ``coverage``.\n",
    "                 See Joachimi et al. 2020.\n",
    "                 The point estimate is the joint posterior MAP.\n",
    "        tail CI: This is the usual quantile CI. I.e., for CI (l,u) and\n",
    "                 coverage c, P(x<l) = (1-c)/2 and P(x>u) = 1-(1-c)/2.\n",
    "                 The point estimate is the median.\n",
    "        std:     Compute the CI as (mean - n_sigma*std, mean + n_sigma*std).\n",
    "                 ``n_sigma`` is the number of standard devations that cover\n",
    "                 ``coverage`` in a normal distribution.\n",
    "                 The point estimate is the mean.\n",
    "        HPD:     Compute the HPDI of the samples.\n",
    "                 The point estimate is the MAP.\n",
    "    samples : array\n",
    "        Samples to use.\n",
    "    weights : array, optional\n",
    "        Sample weights.\n",
    "    coverage : float, optional\n",
    "        Target coverage. This gets converted into sigmas. Default: 0.683.\n",
    "    logpost : array, optional\n",
    "        Array of the log posterior values of the samples. Required for method ``PJ-HPD``.\n",
    "    logpost_sort_idx : array, optional\n",
    "        Array of indices that sort the samples in descending posterior value.\n",
    "        If method is ``PJ-HPD`` and it is not provided, this will be computed\n",
    "        internally from logpost.\n",
    "    return_point_estimate : bool, optional\n",
    "        Whether to return the point_estimate.\n",
    "    return_coverage : bool, optional\n",
    "        Whether to return the actual coverage of the CI.\n",
    "    options : dict, optional\n",
    "        Additional options passed to the CI methods.\n",
    "    Returns\n",
    "    -------\n",
    "    (l, u) : tuple\n",
    "        Credible interval of the samples.\n",
    "    p : float\n",
    "        Point estimate. Only returned if return_point_estimate is true.\n",
    "    coverage : float\n",
    "        The achieved coverage of the returned CI.\n",
    "    \"\"\"\n",
    "    options = options or {}\n",
    "\n",
    "    extras = None\n",
    "    if method.lower() == \"pj-hpd\" or method.lower() == \"projected joint hpd\":\n",
    "        if logpost is None and logpost_sort_idx is None:\n",
    "            raise ValueError(\"For method PJ-HPD, either logpost or logpost_sort_idx need to be specified.\")\n",
    "            \n",
    "        CI, MAP, alpha, n_sample = find_projected_joint_HPDI(samples, weights, \n",
    "                                                             coverage_1d_threshold=coverage,\n",
    "                                                             sort_idx=logpost_sort_idx, log_posterior=logpost, \n",
    "                                                             return_map=True, return_coverage_1d=True, return_n_sample=True,\n",
    "                                                             **options)\n",
    "        point_estimate = MAP\n",
    "        extras = n_sample\n",
    "        \n",
    "    elif method.lower() == \"hpd\" or method.lower() == \"m-hpd\":\n",
    "        CI, marg_MAP, alpha, no_constraints = find_marginal_HPDI(samples, weights, coverage=coverage, \n",
    "                                                                 return_map=True, return_coverage=True,\n",
    "                                                                 check_prior_edges=True,\n",
    "                                                                 **options)\n",
    "        point_estimate = marg_MAP\n",
    "        extras = no_constraints\n",
    "        \n",
    "    elif method.lower() == \"tail ci\" or method.lower() == \"quantile ci\":\n",
    "        CI, marg_median, alpha = find_quantile_CI(samples, weights, coverage=coverage, \n",
    "                                                  return_median=True, return_coverage=True)\n",
    "        point_estimate = marg_median\n",
    "\n",
    "    elif method.lower() == \"std\":\n",
    "        CI, marg_mean, alpha = find_std_CI(samples, weights, coverage=coverage, \n",
    "                                           return_mean=True, return_coverage=True)\n",
    "        point_estimate = marg_mean\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(f\"Method {method} not supported.\")\n",
    "        \n",
    "    result = [CI]\n",
    "    if return_point_estimate:\n",
    "        result += [point_estimate]\n",
    "    if return_coverage:\n",
    "        result += [alpha]\n",
    "    if return_extras:\n",
    "        result += [extras]\n",
    "\n",
    "    if len(result) == 1: \n",
    "        # Only CI\n",
    "        return result[0]\n",
    "    else:\n",
    "        return tuple(result)\n",
    "\n",
    "\n",
    "def find_std_CI(samples, weights=None, coverage=0.683, \n",
    "                return_mean=False, return_coverage=False):\n",
    "    \"\"\"Find mean Â± std of samples and compute the coverage of that interval.\n",
    "    Arguments\n",
    "    ---------\n",
    "    samples : array\n",
    "        Samples to use.\n",
    "    weights : array, optional\n",
    "        Sample weights.\n",
    "    coverage : float, optional\n",
    "        Target coverage. This gets converted into sigmas. Default: 0.683.\n",
    "    return_mean : bool, optional\n",
    "        Whether to return the mean.\n",
    "    return_coverage : bool, optional\n",
    "        Whether to return the actual coverage of the CI.\n",
    "    Returns\n",
    "    -------\n",
    "    (l, u) : tuple\n",
    "        The CI given by (mean - n_sigma*std, mean + n_sigma*std).\n",
    "    mean : float\n",
    "        Mean of the samples. Only returned if return_mean is true.\n",
    "    coverage : float\n",
    "        Coverage of the CI.\n",
    "    \"\"\"\n",
    "    if samples.ndim != 1:\n",
    "        raise ValueError(\"Sample array not 1d.\")\n",
    "\n",
    "    std = np.sqrt(np.cov(samples, aweights=weights, ddof=1))\n",
    "    mean = np.average(samples, weights=weights)\n",
    "\n",
    "    n_sigma = -scipy.stats.norm.ppf((1-coverage)/2)\n",
    "    l = mean - n_sigma*std\n",
    "    u = mean + n_sigma*std\n",
    "\n",
    "    result = [(l, u)]\n",
    "    if return_mean:\n",
    "        result += [mean]\n",
    "    if return_coverage:\n",
    "        cdf = create_interpolated_cdf(samples, weights, threshold=samples.min())\n",
    "        alpha = cdf(u) - cdf(l)\n",
    "        result += [alpha]\n",
    "\n",
    "    if len(result) == 1:\n",
    "        # Only CI\n",
    "        return result[0]\n",
    "    else:\n",
    "        return tuple(result)\n",
    "\n",
    "\n",
    "def find_quantile_CI(samples, weights=None, coverage=0.683, \n",
    "                     return_median=False, return_coverage=False):\n",
    "    \"\"\"Find quantile/tail credible interval.\n",
    "    Arguments\n",
    "    ---------\n",
    "    samples : array\n",
    "        Samples to use.\n",
    "    weights : array, optional\n",
    "        Sample weights. If not specifed is set to 1/len(samples).\n",
    "    coverage : float\n",
    "        Target coverage of the returned interval.\n",
    "    return_median : bool, optional\n",
    "        Whether to return the median.\n",
    "    return_coverage : bool, optional\n",
    "        Whether to return the actual coverage of the CI.\n",
    "    Returns\n",
    "    -------\n",
    "    (l, u) : tuple\n",
    "        The quantile/tail credible interval.\n",
    "    median : float\n",
    "        Median of the samples. Only returned if return_median is true.\n",
    "    coverage : float\n",
    "        The achieved coverage of the returned CI.\n",
    "    \"\"\"\n",
    "    if coverage <= 0.0 or coverage >= 1.0:\n",
    "        raise ValueError(\"Requested coverage outside of range (0,1)\")\n",
    "\n",
    "    if weights is None:\n",
    "        weights = np.ones_like(samples)/samples.size\n",
    "\n",
    "    samples_min = samples.min().squeeze()\n",
    "    samples_max = samples.max().squeeze()\n",
    "\n",
    "    cdf = create_interpolated_cdf(samples, weights, threshold=samples.min())\n",
    "\n",
    "    # Get lower interval bound\n",
    "    c = (1-coverage)/2\n",
    "    res = scipy.optimize.root_scalar(lambda x: cdf(x)-c, bracket=(samples_min, samples_max))\n",
    "    if not res.converged:\n",
    "        raise RuntimeError(f\"Failed to find lower bound for tail coverage {c}.\")\n",
    "    l = res.root\n",
    "\n",
    "    # Get upper interval bound\n",
    "    c = 1-(1-coverage)/2\n",
    "    res = scipy.optimize.root_scalar(lambda x: cdf(x)-c, bracket=(samples_min, samples_max))\n",
    "    if not res.converged:\n",
    "        raise RuntimeError(f\"Failed to find upper bound for tail coverage {c}.\")\n",
    "    u = res.root\n",
    "\n",
    "    alpha = cdf(u) - cdf(l)\n",
    "\n",
    "    result = [(l, u)]\n",
    "    if return_median:\n",
    "        res = scipy.optimize.root_scalar(lambda x: cdf(x)-0.5, bracket=(samples_min, samples_max))\n",
    "        if not res.converged:\n",
    "            raise RuntimeError(f\"Failed to find median.\")\n",
    "        median = res.root\n",
    "        result += [median] \n",
    "    if return_coverage:\n",
    "        result += [alpha]\n",
    "\n",
    "    if len(result) == 1:\n",
    "        # Only CI\n",
    "        return result[0]\n",
    "    else:\n",
    "        return tuple(result)\n",
    "\n",
    "\n",
    "\n",
    "def find_marginal_HPDI(samples, weights=None, coverage=0.683,\n",
    "                       kde_bandwidth=\"silverman\",\n",
    "                       return_map=False, return_coverage=False,\n",
    "                       check_prior_edges=False, prior_edge_threshold=0.1):\n",
    "    \"\"\"Find the highest posterior density interval of a marginal posterior.\n",
    "    Arguments\n",
    "    ---------\n",
    "    samples : array\n",
    "        Array of the samples.\n",
    "    weights : array, optional\n",
    "        Sample weights.\n",
    "    coverage : float\n",
    "        Coverage of the returned CI. Default 0.683.\n",
    "    kde_bandwidth : str or float\n",
    "        Bandwidth used for KDE. Same as scipy.stats.gaussian_kde.\n",
    "    return_map : bool, optional\n",
    "        Whether to return the marginal MAP.\n",
    "    return_coverage : bool, optional\n",
    "        Whether to return the actual coverage of the CI.\n",
    "    Returns\n",
    "    -------\n",
    "    (l, u) : tuple\n",
    "        HPD credible interval.\n",
    "    map : float\n",
    "        MAP of the samples. Only returned if return_map is true.\n",
    "    alpha : float\n",
    "        Achieved coverage of the CI. Only returned if return_coverage is true.\n",
    "    Based on Chieh-An Lin's code.\n",
    "    \"\"\"\n",
    "    if coverage <= 0.0 or coverage >= 1.0:\n",
    "        raise ValueError(\"Requested coverage outside of range (0,1)\")\n",
    "\n",
    "    if samples.ndim != 1:\n",
    "        raise ValueError(\"Samples need to be 1D marginals.\")\n",
    "    if weights is not None and weights.shape != samples.shape:\n",
    "        raise ValueError(f\"Samples and weights have mismatching shapes: {samples.shape} vs {weights.shape}.\")\n",
    "\n",
    "    samples_min = samples.min().squeeze()\n",
    "    samples_max = samples.max().squeeze()\n",
    "\n",
    "    kde = scipy.stats.gaussian_kde(samples, bw_method=kde_bandwidth, weights=weights)\n",
    "\n",
    "    # Find maximum of marginal posterior\n",
    "    res = scipy.optimize.minimize_scalar(lambda x: -kde(x), bracket=(samples_min, samples_max))\n",
    "    if not res.success:\n",
    "        raise RuntimeError(f\"Failed to find maximum marginal posterior: {res}\")\n",
    "\n",
    "    marg_MAP = res.x.squeeze()\n",
    "    marg_MAP_value = -res.fun.squeeze()\n",
    "\n",
    "    marg_samples_min = kde(samples_min)\n",
    "    marg_samples_max = kde(samples_max)\n",
    "\n",
    "    # Find CI\n",
    "    def get_coverage(level, return_interval=False):        \n",
    "        # Find locations of marginal posterior where P(x) = level\n",
    "        if level < marg_samples_min:\n",
    "            l = samples_min\n",
    "        else:\n",
    "            res = scipy.optimize.root_scalar(lambda x: kde(x)-level, \n",
    "                                             bracket=(samples_min, marg_MAP))\n",
    "            if not res.converged:\n",
    "                raise RuntimeError(f\"Failed to find lower bound for level {level}.\")\n",
    "            l = res.root\n",
    "\n",
    "        if level < marg_samples_max:\n",
    "            u = samples_max\n",
    "        else:\n",
    "            res = scipy.optimize.root_scalar(lambda x: kde(x)-level, \n",
    "                                             bracket=(marg_MAP, samples_max))\n",
    "            if not res.converged:\n",
    "                raise RuntimeError(f\"Failed to find upper bound for level {level}.\")\n",
    "            u = res.root\n",
    "\n",
    "        # Get coverage between l and u\n",
    "        alpha = kde.integrate_box_1d(l, u)\n",
    "\n",
    "        if return_interval:\n",
    "            return alpha, (l, u)\n",
    "        else:\n",
    "            return alpha\n",
    "\n",
    "    res = scipy.optimize.root_scalar(lambda x: get_coverage(x)-coverage, \n",
    "                                     bracket=(0, marg_MAP_value))\n",
    "    if not res.converged:\n",
    "        raise RuntimeError(f\"Failed to find coverage.\")\n",
    "\n",
    "    level = res.root\n",
    "    alpha, CI = get_coverage(level, return_interval=True)\n",
    "\n",
    "    if check_prior_edges:\n",
    "        if marg_samples_min/marg_MAP_value > prior_edge_threshold or marg_samples_max/marg_MAP_value > prior_edge_threshold:\n",
    "            prior_edge_hit = True\n",
    "        else:\n",
    "            prior_edge_hit = False\n",
    "\n",
    "    result = [CI]\n",
    "    if return_map:\n",
    "        result += [marg_MAP]\n",
    "    if return_coverage:\n",
    "        result += [alpha]\n",
    "    if check_prior_edges:\n",
    "        result += [prior_edge_hit]\n",
    "\n",
    "    if len(result) == 1:\n",
    "        # Only CI\n",
    "        return result[0]\n",
    "    else:\n",
    "        return tuple(result)\n",
    "\n",
    "\n",
    "def compute_sample_range_and_coverage(samples, weights, idx):\n",
    "    \"\"\"Compute range of values of samples[idx] and their coverage.\"\"\"\n",
    "    l, u = samples[idx].min(), samples[idx].max()\n",
    "    coverage_1d = np.sum(weights[(l <= samples) & (samples <= u)])\n",
    "    coverage_nd = np.sum(weights[idx])\n",
    "    return (l,u), coverage_1d, coverage_nd\n",
    "\n",
    "def create_interpolated_cdf(samples, weights, \n",
    "                            threshold=None, reverse=False,\n",
    "                            start_at_zero=True):\n",
    "    unique_samples, unique_inverse_idx = np.unique(samples, return_inverse=True)\n",
    "    if len(unique_samples) != len(samples):\n",
    "        unique_weights = np.zeros_like(unique_samples)\n",
    "        np.add.at(unique_weights, unique_inverse_idx, weights)\n",
    "        samples = unique_samples\n",
    "        weights = unique_weights\n",
    "\n",
    "    if threshold is None:\n",
    "        selection = np.ones_like(samples, dtype=bool)\n",
    "    else:\n",
    "        selection = samples <= threshold if reverse else samples >= threshold\n",
    "    if reverse:\n",
    "        sample_sort_idx = np.argsort(samples[selection])\n",
    "        cdf = np.cumsum(weights[selection][sample_sort_idx])\n",
    "        cdf = cdf[-1] - cdf\n",
    "        if not start_at_zero:\n",
    "            cdf += weights[selection][sample_sort_idx[-1]]\n",
    "    else:\n",
    "        sample_sort_idx = np.argsort(samples[selection])\n",
    "        cdf = np.cumsum(weights[selection][sample_sort_idx])\n",
    "        if start_at_zero:\n",
    "            cdf -= cdf[0]\n",
    "    \n",
    "    if len(cdf) < 2:\n",
    "        # Only one sample\n",
    "        return lambda x: np.zeros_like(x)\n",
    "\n",
    "    cdf_func = scipy.interpolate.InterpolatedUnivariateSpline(x=samples[selection][sample_sort_idx],\n",
    "                                                              y=cdf, k=1, ext=3)\n",
    "    return cdf_func\n",
    "\n",
    "def weighted_median(a, weights):\n",
    "    if len(a) == 1:\n",
    "        return a\n",
    "    if np.all(np.isclose(a, a[0])):\n",
    "        return a[0]\n",
    "\n",
    "    if weights.sum()/weights.max()-1 < 1e-2:\n",
    "        return a[np.argmax(weights)]\n",
    "\n",
    "    sort_idx = np.argsort(a)\n",
    "    \n",
    "    l_cum = create_interpolated_cdf(a, weights, start_at_zero=False)\n",
    "    r_cum = create_interpolated_cdf(a, weights, reverse=True, start_at_zero=False)\n",
    "    \n",
    "    return scipy.optimize.root_scalar(lambda x: l_cum(x) - r_cum(x), bracket=(a.min(),a.max())).root\n",
    "\n",
    "\n",
    "def find_projected_joint_HPDI(samples, weights=None, coverage_1d_threshold=0.683, \n",
    "                              MAP=None,\n",
    "                              sort_idx=None, log_posterior=None, \n",
    "                              method=\"interpolate\", twosided=True, verbose=False, strict=False,\n",
    "                              return_map=False, return_coverage_1d=False,\n",
    "                              return_coverage_nd=False, return_n_sample=False):\n",
    "    \"\"\"Find the highest posterior density credible interval.\n",
    "    Finds the HPD CI of samples, using the posterior masses given my weights. \n",
    "    The result is interpolated between the two samples that lie on either side\n",
    "    of the target coverage. For example, if the covarage of the n samples with\n",
    "    the highest posterior density is 67% and the covarage of the n+1 samples \n",
    "    is 72%, the result is interpolated to the target coverage (e.g., 68%).\n",
    "    Arguments\n",
    "    ---------\n",
    "    samples : numpy.array\n",
    "        Samples of the parameter to compute the CI for.\n",
    "    weights : numpy.array\n",
    "        Posterior mass of the samples used to compute the coverage. If not \n",
    "        provided is assumed to be unifrom 1/n_sample.\n",
    "    coverage_1d_threshold : float\n",
    "        Target 1D coverage of the CI. (Default 0.683)\n",
    "    MAP : float, optional\n",
    "        If the exact MAP is known, it can be specified here. If not provided, \n",
    "        the sample with the highest posterior value is used. \n",
    "    sort_idx : numpy.array\n",
    "        Array of indices that sort the samples in descending posterior value.\n",
    "        If not provided, this will be computed internally from log_posterior.\n",
    "    log_posterior : numpy.array\n",
    "        Array of the log posterior values of the samples.\n",
    "    method : str\n",
    "        How to interplate the MHPD CI for finite number of samples. Options are\n",
    "        'interpolate', 'expand', 'expand symmetric', 'expand minimal'.\n",
    "        Default 'interpolate'\n",
    "    twosided: bool\n",
    "        Whether to keep search samples until both the lower and upper bounds\n",
    "        extend beyond the threshold coverage. Default: True.\n",
    "    verbose : bool\n",
    "        Print extra outputs.\n",
    "    strict : bool\n",
    "        Whether to raise an exception of no CI can be found. Default: False.\n",
    "    return_map : bool, optional\n",
    "        Whether to return the MAP of the samples.\n",
    "    return_coverage_1d : bool, optional\n",
    "        Whether to return the actual, 1D coverage of the CI.\n",
    "    return_coverage_nd : bool, optional\n",
    "        Whether to return the nd coverage of the HPD used to compute the CI.\n",
    "    return_n_sample : bool, optional\n",
    "        Whether to return the number of (joint posterior) samples used to compute the CI.\n",
    "    Returns\n",
    "    -------\n",
    "    (l, u) : tuple\n",
    "        CI of samples.\n",
    "    coverage_1d : float\n",
    "        Achieved 1D coverage.\n",
    "    coverage_nd : float\n",
    "        N-dimensional coverage of the samples in CI.\n",
    "    n_sample : int\n",
    "        Number of samples in CI.\n",
    "    \"\"\"\n",
    "    if coverage_1d_threshold <= 0.0 or coverage_1d_threshold >= 1.0:\n",
    "        raise ValueError(\"Requested coverage outside of range (0,1)\")\n",
    "\n",
    "    if weights is None:\n",
    "        weights = np.ones_like(samples)/samples.size\n",
    "    if sort_idx is None:\n",
    "        if log_posterior is None:\n",
    "            raise ValueError(\"If sorting indicies are not provided, the log posterior must be given.\")\n",
    "        sort_idx = np.argsort(log_posterior)[::-1]\n",
    "\n",
    "    if MAP is not None:\n",
    "        samples = np.insert(samples, 0, MAP)\n",
    "        weights = np.insert(weights, 0, 0.0)\n",
    "        sort_idx = np.insert(sort_idx + 1, 0, 0)\n",
    "\n",
    "    if samples[sort_idx[0]] <= samples.min():\n",
    "        samples[sort_idx[0]] = samples.min()\n",
    "        twosided = False\n",
    "        warnings.warn(\"The starting point is at or lower than the sample range. Only using one sided interpolation.\")\n",
    "    elif samples[sort_idx[0]] >= samples.max():\n",
    "        samples[sort_idx[0]] = samples.max()\n",
    "        twosided = False\n",
    "        warnings.warn(\"The starting point is at or higher than the sample range. Only using one sided interpolation.\")\n",
    "\n",
    "\n",
    "    l_outer = l_inner = u_inner = u_outer = samples[sort_idx[0]]\n",
    "    coverage_1d_inner = coverage_nd_inner = 0\n",
    "    n_inner = 0\n",
    "    for i in range(2, len(sort_idx)):\n",
    "        (l_this, u_this), coverage_1d_this, coverage_nd_this = compute_sample_range_and_coverage(samples, weights, sort_idx[:i])\n",
    "\n",
    "        if coverage_1d_this < coverage_1d_threshold:\n",
    "            # Still under the target coverage\n",
    "            l_inner = l_outer = l_this\n",
    "            u_inner = u_outer = u_this\n",
    "            coverage_1d_inner = coverage_1d_this\n",
    "            coverage_nd_inner = coverage_nd_this\n",
    "            n_inner = i\n",
    "        else:\n",
    "            # Over the target coverage. Only update outer limits\n",
    "            if l_this < l_inner:\n",
    "                l_outer = l_this\n",
    "            if u_this > u_inner:\n",
    "                u_outer = u_this\n",
    "\n",
    "        if method == \"interpolate\":\n",
    "            if (twosided and (l_inner != l_outer and u_inner != u_outer)) \\\n",
    "            or (not twosided and (l_inner != l_outer or u_inner != u_outer)):\n",
    "                # Got two estimates for both sides\n",
    "\n",
    "                l_cdf_func = create_interpolated_cdf(samples, weights, l_inner, reverse=True)\n",
    "                u_cdf_func = create_interpolated_cdf(samples, weights, u_inner, reverse=False)\n",
    "\n",
    "                def coverage(t, l_cdf, u_cdf):\n",
    "                    l = l_outer*(1-t) + l_inner*t\n",
    "                    u = u_outer*(1-t) + u_inner*t\n",
    "                    return coverage_1d_inner + l_cdf(l) + u_cdf(u)\n",
    "\n",
    "                res = scipy.optimize.root_scalar(f=lambda t, l_cdf=l_cdf_func, u_cdf=u_cdf_func: coverage(t, l_cdf, u_cdf)-coverage_1d_threshold,\n",
    "                                                 bracket=(0,1))\n",
    "                t = res.root\n",
    "                l = l_outer*(1-t) + l_inner*t\n",
    "                u = u_outer*(1-t) + u_inner*t\n",
    "                coverage_1d = np.sum(weights[(l <= samples) & (samples <= u)])\n",
    "                break\n",
    "\n",
    "        elif method == \"expand symmetric\":\n",
    "            if(l_inner != l_outer or u_inner != u_outer):\n",
    "                delta_init = max(l_inner-l_outer, u_outer-u_inner)\n",
    "\n",
    "                l_cdf_func = create_interpolated_cdf(samples, weights, l_inner, reverse=True)\n",
    "                u_cdf_func = create_interpolated_cdf(samples, weights, u_inner, reverse=False)\n",
    "\n",
    "                def coverage(d, l_cdf, u_cdf):\n",
    "                    l = l_inner - d\n",
    "                    u = u_inner + d\n",
    "                    return coverage_1d_inner + l_cdf(l) + u_cdf(u)\n",
    "\n",
    "                res = scipy.optimize.root_scalar(f=lambda t, l_cdf=l_cdf_func, u_cdf=u_cdf_func: coverage(t, l_cdf, u_cdf)-coverage_1d_threshold,\n",
    "                                                 bracket=(0,delta_init))\n",
    "                d = res.root\n",
    "                l = l_inner - d\n",
    "                u = u_inner + d\n",
    "                coverage_1d = np.sum(weights[(l <= samples) & (samples <= u)])\n",
    "                break\n",
    "\n",
    "        elif method == \"expand\":\n",
    "            if(l_inner != l_outer or u_inner != u_outer):\n",
    "                # Only need two estimates for one side.\n",
    "\n",
    "                delta_coverage = (coverage_1d_threshold-coverage_1d_inner)/2\n",
    "                sample_sort_idx = np.argsort(samples[samples < l_inner])[::-1]\n",
    "                l_idx = np.searchsorted(np.cumsum(weights[samples < l_inner][sample_sort_idx]), delta_coverage)\n",
    "\n",
    "                if l_idx == samples[samples < l_inner].size:\n",
    "                    l_idx -= 1\n",
    "                l = samples[samples < l_inner][sample_sort_idx][l_idx]\n",
    "\n",
    "                sample_sort_idx = np.argsort(samples[samples > u_inner])\n",
    "                u_idx = np.searchsorted(np.cumsum(weights[samples > u_inner][sample_sort_idx]), delta_coverage)\n",
    "                if u_idx == samples[samples > u_inner].size:\n",
    "                    u_idx -= 1\n",
    "                u = samples[samples > u_inner][sample_sort_idx][u_idx]\n",
    "\n",
    "                coverage_1d = np.sum(weights[(l <= samples) & (samples <= u)])\n",
    "                break\n",
    "\n",
    "        elif method == \"expand minimal\":\n",
    "            if(l_inner != l_outer and u_inner != u_outer):\n",
    "                # Initial guess\n",
    "                l_delta = l_inner-l_outer\n",
    "                u_delta = u_outer-u_inner\n",
    "\n",
    "                l_cdf_func = create_interpolated_cdf(samples, weights, l_inner, reverse=True)\n",
    "                u_cdf_func = create_interpolated_cdf(samples, weights, u_inner, reverse=False)\n",
    "\n",
    "                def constraint(x, l_cdf, u_cdf):\n",
    "                    l, u = x\n",
    "                    coverage = coverage_1d_inner + l_cdf(l) + u_cdf(u)\n",
    "                    return coverage - coverage_1d_threshold\n",
    "\n",
    "                def constraint_jac(x, l_cdf, u_cdf):\n",
    "                    l, u = x\n",
    "                    return l_cdf(l, nu=1), u_cdf(u, nu=1)\n",
    "\n",
    "                def fun(x):\n",
    "                    l, u = x\n",
    "                    return u-l\n",
    "\n",
    "                def fun_jac(x):\n",
    "                    l, u = x\n",
    "                    return -1.0, 1.0\n",
    "\n",
    "                res = scipy.optimize.minimize(fun=fun, x0=[l_inner-l_delta/2, u_inner+u_delta/2],\n",
    "                                              jac=fun_jac,\n",
    "                                              bounds=[(l_outer, l_inner), (u_inner, u_outer)],\n",
    "                                              constraints=[{\"type\" : \"eq\", \"fun\" : constraint, \"jac\" : constraint_jac, \"args\" : (l_cdf_func, u_cdf_func)},\n",
    "                                                          ],\n",
    "                                              options={\"ftol\" : 0.05},\n",
    "                                              method=\"SLSQP\")\n",
    "                if not res.success:\n",
    "                    if res.message != \"Positive directional derivative for linesearch\":\n",
    "                        print(res)\n",
    "\n",
    "                l, u = res.x\n",
    "                coverage_1d = np.sum(weights[(l <= samples) & (samples <= u)])\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"Method {method} not supported.\")      \n",
    "    else:\n",
    "        if strict:\n",
    "            raise RuntimeError(f\"Could not match 1D coverage of {coverage_1d_threshold}. Got coverage of {coverage_1d_this:.2f} for CI ({l_this:.3f}, {u_this:.3f}).\")\n",
    "        else:\n",
    "            warnings.warn(f\"Could not match 1D coverage of {coverage_1d_threshold}. Got coverage of {coverage_1d_this:.2f} for CI ({l_this:.3f}, {u_this:.3f}).\")\n",
    "            l, u = l_this, u_this \n",
    "            coverage_1d = coverage_1d_this\n",
    "\n",
    "    result = [(l, u)]\n",
    "    if return_map:\n",
    "        if MAP is not None:\n",
    "            result += [samples[sort_idx[1]]]\n",
    "        else:\n",
    "            result += [samples[sort_idx[0]]]\n",
    "    if return_coverage_1d:\n",
    "        result += [coverage_1d]\n",
    "    if return_coverage_nd:\n",
    "        result += [coverage_nd_inner]\n",
    "    if return_n_sample:\n",
    "        result += [n_inner]\n",
    "\n",
    "    if len(result) == 1:\n",
    "        # Only CI\n",
    "        return result[0]\n",
    "    else:\n",
    "        return tuple(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMontePythonChain(directory):\n",
    "    data = np.loadtxt(directory + '/chain_NS__accepted.txt')\n",
    "    # load parameter names\n",
    "    filename =  glob.glob(directory + '/*.paramnames')\n",
    "    names = np.loadtxt(filename[0], dtype=str, delimiter='\\t').T[0]\n",
    "    # first two columns: weight & mloglkl\n",
    "    params = np.concatenate((['weight','mloglkl'], names))\n",
    "    # remove trailing spaces\n",
    "    for idx, name in enumerate(params):\n",
    "        if name[-1] == ' ':\n",
    "            params[idx] = name[:-1]\n",
    "    chain = pd.DataFrame(data, columns = params)\n",
    "    # add the true chi2 to the chain\n",
    "    chain.insert(2,'chi2',2*chain['mloglkl'])\n",
    "    return(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi-square values for a gaussian prior\n",
    "means = np.array([0., 0.181, 1.110, 1.395, -1.265])\n",
    "sigmas = np.array([1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "def gaussian_chi2(x, mean, sigma):\n",
    "    return((x - mean)**2 / sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that writes the chain including loglkl, prior, and posterior to a file\n",
    "def add_posterior(input_folder):\n",
    "    chain = loadMontePythonChain(input_folder)\n",
    "    # Calculate the prior from a Gaussian chi-square \n",
    "    prior = 0.5*np.sum(gaussian_chi2(chain[['D_z1','D_z2','D_z3','D_z4','D_z5']].values, means, sigmas), axis=1)\n",
    "    # add prior column to the chain\n",
    "    chain.insert(3, 'prior', prior)\n",
    "    # add posterior column to the chain (post = loglkl + prior) Note: This is the true log-likelihood; not the default -2*loglkl that MontePython puts in the output file!\n",
    "    chain.insert(4,'post', chain['mloglkl'] + chain['prior'])\n",
    "    return(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bestfit_chi2(input_folder):\n",
    "    chain = add_posterior(input_folder)\n",
    "    d = np.where(chain['mloglkl']<10)[0]\n",
    "    chain = chain.drop(d)\n",
    "    chi2 = chain['chi2'][np.where(chain['post'] == np.min(chain['post']))[0]]\n",
    "    return(chi2.values[0])\n",
    "def print_bestfit(input_folder):\n",
    "    chain = add_posterior(input_folder)\n",
    "    bf = chain.loc[np.where(chain['post'] == np.min(chain['post']))[0]]\n",
    "    return(bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1cosmo: 152.338280\n",
      "2cosmo: 150.761880\n"
     ]
    }
   ],
   "source": [
    "# K1K \n",
    "print('1cosmo: %f'%find_bestfit_chi2('../chains/K1K_1c'))\n",
    "print('2cosmo: %f'%find_bestfit_chi2('../chains/K1K_bp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1cosmo: 157.491780\n",
      "2cosmo: 156.363120\n"
     ]
    }
   ],
   "source": [
    "# K1K + clustering\n",
    "print('1cosmo: %f'%find_bestfit_chi2('../chains/K1K_Cl_1c'))\n",
    "print('2cosmo: %f'%find_bestfit_chi2('../chains/K1K_Cl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1cosmo: 161.330220\n",
      "2cosmo: 160.756060\n"
     ]
    }
   ],
   "source": [
    "# K1K + clustering + Lyman-alpha\n",
    "print('1cosmo: %f'%find_bestfit_chi2('../chains/K1K_ClLy_1c'))\n",
    "print('2cosmo: %f'%find_bestfit_chi2('../chains/K1K_ClLy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1cosmo: 163.157360\n",
      "2cosmo: 161.185540\n"
     ]
    }
   ],
   "source": [
    "# K1K + clustering + Lyman-alpha + CMB\n",
    "print('1cosmo: %f'%find_bestfit_chi2('../chains/K1K_ClLyC_1c'))\n",
    "print('2cosmo: %f'%find_bestfit_chi2('../chains/K1K_ClLyC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1cosmo: 152.365620\n",
      "2cosmo: 152.821400\n"
     ]
    }
   ],
   "source": [
    "# K1K + Lyman-alpha\n",
    "print('1cosmo: %f'%find_bestfit_chi2('../chains/K1K_Ly_1c'))\n",
    "print('2cosmo: %f'%find_bestfit_chi2('../chains/K1K_Ly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1cosmo: 151.671480\n",
      "2cosmo: 152.750660\n"
     ]
    }
   ],
   "source": [
    "# K1K + CMB\n",
    "print('1cosmo: %f'%find_bestfit_chi2('../chains/K1K_CMB_1c'))\n",
    "print('2cosmo: %f'%find_bestfit_chi2('../chains/K1K_CMB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "K1K = add_posterior('../chains/K1K_ClLyC_1c')\n",
    "#official_chain = np.loadtxt('../../KiDS1000_cosmis_shear_data_release/chains_and_config_files/main_chains_iterative_covariance/bp/chain/output_multinest_C.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.6908416813341418, 0.712018114979917), 0.7053681)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first two values: upper and lower confidence intervals (68% levels)\n",
    "# last value MAP value (should be equivalent to the value inferred with my own script)\n",
    "find_projected_joint_HPDI(samples=np.array(K1K['h']), weights=np.array(K1K['weight']), log_posterior=-np.array(K1K['post']),return_map=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.7673325619943326, 0.7914235919454251), 0.7755883)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first two values: upper and lower confidence intervals (68% levels)\n",
    "# last value MAP value (should be equivalent to the value inferred with my own script)\n",
    "find_projected_joint_HPDI(samples=np.array(K1K['S_8']), weights=np.array(K1K['weight']), log_posterior=-np.array(K1K['post']),return_map=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'official_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-df8b205abd81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfind_projected_joint_HPDI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mofficial_chain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mofficial_chain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_posterior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mofficial_chain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'official_chain' is not defined"
     ]
    }
   ],
   "source": [
    "find_projected_joint_HPDI(samples=official_chain[:, 4], weights=official_chain[:, -1], log_posterior=official_chain[:, -2],return_map=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
